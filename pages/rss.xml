<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>CV ArXiv Daily</title><link>https://your-github-repo.com</link><description>Daily CV papers from ArXiv</description><language>en-us</language><lastBuildDate>Mon, 16 Jun 2025 10:12:29 GMT</lastBuildDate><item><title>Hydrodynamics of chiral nematics in a channel and sudden contraction geometry</title><link>http://arxiv.org/abs/2506.11933</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Isreal Morawo et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11933'&gt;http://arxiv.org/abs/2506.11933&lt;/a&gt;</description></item><item><title>Universal Scaling Laws for Deep Indentation Beyond the Hertzian Regime</title><link>http://arxiv.org/abs/2506.11461</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Tong Mu et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11461'&gt;http://arxiv.org/abs/2506.11461&lt;/a&gt;</description></item><item><title>EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence</title><link>http://arxiv.org/abs/2506.10600</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Wang Xinjie et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10600'&gt;http://arxiv.org/abs/2506.10600&lt;/a&gt;</description></item><item><title>EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models</title><link>http://arxiv.org/abs/2506.10100</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Yantai Yang et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10100'&gt;http://arxiv.org/abs/2506.10100&lt;/a&gt;</description></item><item><title>Adaptive event-triggered robust tracking control of soft robots</title><link>http://arxiv.org/abs/2506.09523</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Renjie Ma et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09523'&gt;http://arxiv.org/abs/2506.09523&lt;/a&gt;</description></item><item><title>LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments</title><link>http://arxiv.org/abs/2506.07223</link><pubDate>2025.06.08 00:00:00 GMT</pubDate><description>Yangqing Zheng et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.07223'&gt;http://arxiv.org/abs/2506.07223&lt;/a&gt;</description></item><item><title>Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions</title><link>http://arxiv.org/abs/2506.11111</link><pubDate>2025.06.08 00:00:00 GMT</pubDate><description>Kun Zhang et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11111'&gt;http://arxiv.org/abs/2506.11111&lt;/a&gt;</description></item><item><title>A Soft Robotic Module with Pneumatic Actuation and Enhanced Controllability Using a Shape Memory Alloy Wire</title><link>http://arxiv.org/abs/2506.05741</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Mohammadnavid Golchin et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.05741'&gt;http://arxiv.org/abs/2506.05741&lt;/a&gt;</description></item><item><title>NEAT and HyperNEAT based Design for Soft Actuator Controllers</title><link>http://arxiv.org/abs/2506.04698</link><pubDate>2025.06.05 00:00:00 GMT</pubDate><description>Hugo Alcaraz-Herrera et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.04698'&gt;http://arxiv.org/abs/2506.04698&lt;/a&gt;</description></item><item><title>Hierarchical-Task-Aware Multi-modal Mixture of Incremental LoRA Experts for Embodied Continual Learning</title><link>http://arxiv.org/abs/2506.04595</link><pubDate>2025.06.05 00:00:00 GMT</pubDate><description>Ziqi Jia et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.04595'&gt;http://arxiv.org/abs/2506.04595&lt;/a&gt;</description></item><item><title>Multimodal Limbless Crawling Soft Robot with a Kirigami Skin</title><link>http://arxiv.org/abs/2506.04547</link><pubDate>2025.06.05 00:00:00 GMT</pubDate><description>Jonathan Tirado et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.04547'&gt;http://arxiv.org/abs/2506.04547&lt;/a&gt;</description></item><item><title>Designing morphologies of soft medical devices using cooperative neuro coevolution</title><link>http://arxiv.org/abs/2506.03847</link><pubDate>2025.06.04 00:00:00 GMT</pubDate><description>Hugo Alcaraz-Herrera et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.03847'&gt;http://arxiv.org/abs/2506.03847&lt;/a&gt;</description></item><item><title>Design of Trimmed Helicoid Soft-Rigid Hybrid Robots</title><link>http://arxiv.org/abs/2506.03380</link><pubDate>2025.06.03 00:00:00 GMT</pubDate><description>Zach J. Patterson et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.03380'&gt;http://arxiv.org/abs/2506.03380&lt;/a&gt; | Code: &lt;a href='https://github.com/zpatty/sr_helix'&gt;https://github.com/zpatty/sr_helix&lt;/a&gt;</description></item><item><title>Multi-Objective Neural Network Assisted Design Optimization of Soft Fin-Ray Grippers for Enhanced Grasping Performance</title><link>http://arxiv.org/abs/2506.00494</link><pubDate>2025.05.31 00:00:00 GMT</pubDate><description>Ali Ghanizadeh et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.00494'&gt;http://arxiv.org/abs/2506.00494&lt;/a&gt;</description></item><item><title>LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks</title><link>http://arxiv.org/abs/2506.00411</link><pubDate>2025.05.31 00:00:00 GMT</pubDate><description>Yi Yang et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.00411'&gt;http://arxiv.org/abs/2506.00411&lt;/a&gt;</description></item><item><title>Position: Olfaction Standardization is Essential for the Advancement of Embodied Artificial Intelligence</title><link>http://arxiv.org/abs/2506.00398</link><pubDate>2025.05.31 00:00:00 GMT</pubDate><description>Kordel K. France et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.00398'&gt;http://arxiv.org/abs/2506.00398&lt;/a&gt;</description></item><item><title>MagicGripper: A Multimodal Sensor-Integrated Gripper for Contact-Rich Robotic Manipulation</title><link>http://arxiv.org/abs/2505.24382</link><pubDate>2025.05.30 00:00:00 GMT</pubDate><description>Wen Fan et al. | Paper: &lt;a href='http://arxiv.org/abs/2505.24382'&gt;http://arxiv.org/abs/2505.24382&lt;/a&gt;</description></item><item><title>Human sensory-musculoskeletal modeling and control of whole-body movements</title><link>http://arxiv.org/abs/2506.00071</link><pubDate>2025.05.29 00:00:00 GMT</pubDate><description>Chenhui Zuo et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.00071'&gt;http://arxiv.org/abs/2506.00071&lt;/a&gt;</description></item><item><title>LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for Scientific Embodied Agents</title><link>http://arxiv.org/abs/2505.22634</link><pubDate>2025.05.28 00:00:00 GMT</pubDate><description>Rui Li et al. | Paper: &lt;a href='http://arxiv.org/abs/2505.22634'&gt;http://arxiv.org/abs/2505.22634&lt;/a&gt;</description></item><item><title>Soft Electrothermal Meta-Actuator for Robust Multifunctional Control</title><link>http://arxiv.org/abs/2505.21992</link><pubDate>2025.05.28 00:00:00 GMT</pubDate><description>Hanseong Jo et al. | Paper: &lt;a href='http://arxiv.org/abs/2505.21992'&gt;http://arxiv.org/abs/2505.21992&lt;/a&gt;</description></item><item><title>Viscoelasticity of biomimetic scale beams from trapped complex fluids</title><link>http://arxiv.org/abs/2505.21760</link><pubDate>2025.05.27 00:00:00 GMT</pubDate><description>Pranta Rahman Sarkar et al. | Paper: &lt;a href='http://arxiv.org/abs/2505.21760'&gt;http://arxiv.org/abs/2505.21760&lt;/a&gt;</description></item><item><title>MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation</title><link>http://arxiv.org/abs/2505.21483</link><pubDate>2025.05.27 00:00:00 GMT</pubDate><description>Kerui Ren et al. | Paper: &lt;a href='http://arxiv.org/abs/2505.21483'&gt;http://arxiv.org/abs/2505.21483&lt;/a&gt;</description></item><item><title>Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO</title><link>http://arxiv.org/abs/2505.21457</link><pubDate>2025.05.27 00:00:00 GMT</pubDate><description>Muzhi Zhu et al. | Paper: &lt;a href='http://arxiv.org/abs/2505.21457'&gt;http://arxiv.org/abs/2505.21457&lt;/a&gt;</description></item><item><title>RFTF: Reinforcement Fine-tuning for Embodied Agents with Temporal Feedback</title><link>http://arxiv.org/abs/2505.19767</link><pubDate>2025.05.26 00:00:00 GMT</pubDate><description>Junyang Shu et al. | Paper: &lt;a href='http://arxiv.org/abs/2505.19767'&gt;http://arxiv.org/abs/2505.19767&lt;/a&gt;</description></item><item><title>DiffE2E: Rethinking End-to-End Driving with a Hybrid Action Diffusion and Supervised Policy</title><link>http://arxiv.org/abs/2505.19516</link><pubDate>2025.05.26 00:00:00 GMT</pubDate><description>Rui Zhao et al. | Paper: &lt;a href='http://arxiv.org/abs/2505.19516'&gt;http://arxiv.org/abs/2505.19516&lt;/a&gt;</description></item><item><title>Rotational Multi-material 3D Printing of Soft Robotic Matter with Asymmetrical Embedded Pneumatics</title><link>http://arxiv.org/abs/2505.18095</link><pubDate>2025.05.23 00:00:00 GMT</pubDate><description>Jackson K. Wilt et al. | Paper: &lt;a href='http://arxiv.org/abs/2505.18095'&gt;http://arxiv.org/abs/2505.18095&lt;/a&gt;</description></item><item><title>DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation</title><link>http://arxiv.org/abs/2505.18078</link><pubDate>2025.05.23 00:00:00 GMT</pubDate><description>Junhao Chen et al. | Paper: &lt;a href='http://arxiv.org/abs/2505.18078'&gt;http://arxiv.org/abs/2505.18078&lt;/a&gt;</description></item><item><title>BEDI: A Comprehensive Benchmark for Evaluating Embodied Agents on UAVs</title><link>http://arxiv.org/abs/2505.18229</link><pubDate>2025.05.23 00:00:00 GMT</pubDate><description>Mingning Guo et al. | Paper: &lt;a href='http://arxiv.org/abs/2505.18229'&gt;http://arxiv.org/abs/2505.18229&lt;/a&gt; | Code: &lt;a href='https://github.com/lostwolves/bedi'&gt;https://github.com/lostwolves/bedi&lt;/a&gt;</description></item><item><title>HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning</title><link>http://arxiv.org/abs/2505.17645</link><pubDate>2025.05.23 00:00:00 GMT</pubDate><description>Chuhao Zhou et al. | Paper: &lt;a href='http://arxiv.org/abs/2505.17645'&gt;http://arxiv.org/abs/2505.17645&lt;/a&gt;</description></item><item><title>MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos</title><link>http://arxiv.org/abs/2505.11868</link><pubDate>2025.05.17 00:00:00 GMT</pubDate><description>Hongyi Zhou et al. | Paper: &lt;a href='http://arxiv.org/abs/2505.11868'&gt;http://arxiv.org/abs/2505.11868&lt;/a&gt;</description></item><item><title>SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies</title><link>http://arxiv.org/abs/2506.11948</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Nadun Ranawaka Arachchige et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11948'&gt;http://arxiv.org/abs/2506.11948&lt;/a&gt;</description></item><item><title>mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity</title><link>http://arxiv.org/abs/2506.11916</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Elvis Nava et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11916'&gt;http://arxiv.org/abs/2506.11916&lt;/a&gt;</description></item><item><title>ExoStart: Efficient learning for dexterous manipulation with sensorized exoskeleton demonstrations</title><link>http://arxiv.org/abs/2506.11775</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Zilin Si et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11775'&gt;http://arxiv.org/abs/2506.11775&lt;/a&gt;</description></item><item><title>Control Architecture and Design for a Multi-robotic Visual Servoing System in Automated Manufacturing Environment</title><link>http://arxiv.org/abs/2506.11387</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Rongfei Li et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11387'&gt;http://arxiv.org/abs/2506.11387&lt;/a&gt;</description></item><item><title>Influence Functions for Data Attribution in Linear System Identification and LQR Control</title><link>http://arxiv.org/abs/2506.11293</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Jiachen Li et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11293'&gt;http://arxiv.org/abs/2506.11293&lt;/a&gt;</description></item><item><title>Gondola: Grounded Vision Language Planning for Generalizable Robotic Manipulation</title><link>http://arxiv.org/abs/2506.11261</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Shizhe Chen et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11261'&gt;http://arxiv.org/abs/2506.11261&lt;/a&gt;</description></item><item><title>Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop</title><link>http://arxiv.org/abs/2506.10968</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Justin Kerr et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10968'&gt;http://arxiv.org/abs/2506.10968&lt;/a&gt;</description></item><item><title>GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation</title><link>http://arxiv.org/abs/2506.10966</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Ning Gao et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10966'&gt;http://arxiv.org/abs/2506.10966&lt;/a&gt;</description></item><item><title>Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material</title><link>http://arxiv.org/abs/2506.10875</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Guanjin Wang et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10875'&gt;http://arxiv.org/abs/2506.10875&lt;/a&gt;</description></item><item><title>RationalVLA: A Rational Vision-Language-Action Model with Dual System</title><link>http://arxiv.org/abs/2506.10826</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Wenxuan Song et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10826'&gt;http://arxiv.org/abs/2506.10826&lt;/a&gt;</description></item><item><title>Human-Robot Navigation using Event-based Cameras and Reinforcement Learning</title><link>http://arxiv.org/abs/2506.10790</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Ignacio Bugueno-Cordova et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10790'&gt;http://arxiv.org/abs/2506.10790&lt;/a&gt;</description></item><item><title>RICE: Reactive Interaction Controller for Cluttered Canopy Environment</title><link>http://arxiv.org/abs/2506.10383</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Nidhi Homey Parayil et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10383'&gt;http://arxiv.org/abs/2506.10383&lt;/a&gt;</description></item><item><title>Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success</title><link>http://arxiv.org/abs/2506.10359</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Che Wang et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10359'&gt;http://arxiv.org/abs/2506.10359&lt;/a&gt;</description></item><item><title>A Novel Feedforward Youla Parameterization Method for Avoiding Local Minima in Stereo Image Based Visual Servoing Control</title><link>http://arxiv.org/abs/2506.10252</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Rongfei Li et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10252'&gt;http://arxiv.org/abs/2506.10252&lt;/a&gt;</description></item><item><title>Innovative Adaptive Imaged Based Visual Servoing Control of 6 DoFs Industrial Robot Manipulators</title><link>http://arxiv.org/abs/2506.10240</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Rongfei Li et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10240'&gt;http://arxiv.org/abs/2506.10240&lt;/a&gt;</description></item><item><title>One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture</title><link>http://arxiv.org/abs/2506.10106</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Marcos Abel Zuzuárregui et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10106'&gt;http://arxiv.org/abs/2506.10106&lt;/a&gt;</description></item><item><title>EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models</title><link>http://arxiv.org/abs/2506.10100</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Yantai Yang et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10100'&gt;http://arxiv.org/abs/2506.10100&lt;/a&gt;</description></item><item><title>eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures</title><link>http://arxiv.org/abs/2506.09994</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Venkatesh Pattabiraman et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09994'&gt;http://arxiv.org/abs/2506.09994&lt;/a&gt;</description></item><item><title>Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation</title><link>http://arxiv.org/abs/2506.09990</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Wenbo Zhang et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09990'&gt;http://arxiv.org/abs/2506.09990&lt;/a&gt;</description></item><item><title>SAFE: Multitask Failure Detection for Vision-Language-Action Models</title><link>http://arxiv.org/abs/2506.09937</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Qiao Gu et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09937'&gt;http://arxiv.org/abs/2506.09937&lt;/a&gt;</description></item><item><title>From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models</title><link>http://arxiv.org/abs/2506.09930</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Irving Fang et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09930'&gt;http://arxiv.org/abs/2506.09930&lt;/a&gt;</description></item><item><title>Hierarchical Learning-Enhanced MPC for Safe Crowd Navigation with Heterogeneous Constraints</title><link>http://arxiv.org/abs/2506.09859</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Huajian Liu et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09859'&gt;http://arxiv.org/abs/2506.09859&lt;/a&gt;</description></item><item><title>Reinforced Refinement with Self-Aware Expansion for End-to-End Autonomous Driving</title><link>http://arxiv.org/abs/2506.09800</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Haochen Liu et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09800'&gt;http://arxiv.org/abs/2506.09800&lt;/a&gt;</description></item><item><title>CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings</title><link>http://arxiv.org/abs/2506.09699</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Mattia Nardon et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09699'&gt;http://arxiv.org/abs/2506.09699&lt;/a&gt;</description></item><item><title>Understanding the Performance and Power of LLM Inferencing on Edge Accelerators</title><link>http://arxiv.org/abs/2506.09554</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Mayank Arya et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09554'&gt;http://arxiv.org/abs/2506.09554&lt;/a&gt;</description></item><item><title>Advances on Affordable Hardware Platforms for Human Demonstration Acquisition in Agricultural Applications</title><link>http://arxiv.org/abs/2506.09494</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Alberto San-Miguel-Tello et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09494'&gt;http://arxiv.org/abs/2506.09494&lt;/a&gt;</description></item><item><title>DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects</title><link>http://arxiv.org/abs/2506.09491</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Guanghu Xie et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09491'&gt;http://arxiv.org/abs/2506.09491&lt;/a&gt;</description></item><item><title>Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation</title><link>http://arxiv.org/abs/2506.09422</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Ye Niu et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09422'&gt;http://arxiv.org/abs/2506.09422&lt;/a&gt;</description></item><item><title>Analyzing Key Objectives in Human-to-Robot Retargeting for Dexterous Manipulation</title><link>http://arxiv.org/abs/2506.09384</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Chendong Xin et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09384'&gt;http://arxiv.org/abs/2506.09384&lt;/a&gt;</description></item><item><title>ContextBuddy: AI-Enhanced Contextual Insights for Security Alert Investigation (Applied to Intrusion Detection)</title><link>http://arxiv.org/abs/2506.09365</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Ronal Singh et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09365'&gt;http://arxiv.org/abs/2506.09365&lt;/a&gt;</description></item><item><title>Affogato: Learning Open-Vocabulary Affordance Grounding with Automated Data Generation at Scale</title><link>http://arxiv.org/abs/2506.12009</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Junha Lee et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.12009'&gt;http://arxiv.org/abs/2506.12009&lt;/a&gt;</description></item><item><title>SIMSHIFT: A Benchmark for Adapting Neural Surrogates to Distribution Shifts</title><link>http://arxiv.org/abs/2506.12007</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Paul Setinek et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.12007'&gt;http://arxiv.org/abs/2506.12007&lt;/a&gt;</description></item><item><title>crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation Techniques for Vestibular Schwannoma and Cochlea Segmentation from 2021 to 2023</title><link>http://arxiv.org/abs/2506.12006</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Navodini Wijethilake et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.12006'&gt;http://arxiv.org/abs/2506.12006&lt;/a&gt;</description></item><item><title>Preserving Clusters in Prompt Learning for Unsupervised Domain Adaptation</title><link>http://arxiv.org/abs/2506.11493</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Tung-Long Vuong et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11493'&gt;http://arxiv.org/abs/2506.11493&lt;/a&gt;</description></item><item><title>MUDAS: Mote-scale Unsupervised Domain Adaptation in Multi-label Sound Classification</title><link>http://arxiv.org/abs/2506.11331</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Jihoon Yun et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11331'&gt;http://arxiv.org/abs/2506.11331&lt;/a&gt;</description></item><item><title>BioClinical ModernBERT: A State-of-the-Art Long-Context Encoder for Biomedical and Clinical NLP</title><link>http://arxiv.org/abs/2506.10896</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Thomas Sounack et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10896'&gt;http://arxiv.org/abs/2506.10896&lt;/a&gt;</description></item><item><title>Evaluating Large Language Models on Non-Code Software Engineering Tasks</title><link>http://arxiv.org/abs/2506.10833</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Fabian C. Peña et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10833'&gt;http://arxiv.org/abs/2506.10833&lt;/a&gt;</description></item><item><title>ConStyX: Content Style Augmentation for Generalizable Medical Image Segmentation</title><link>http://arxiv.org/abs/2506.10675</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Xi Chen et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10675'&gt;http://arxiv.org/abs/2506.10675&lt;/a&gt;</description></item><item><title>Teaching in adverse scenes: a statistically feedback-driven threshold and mask adjustment teacher-student framework for object detection in UAV images under adverse scenes</title><link>http://arxiv.org/abs/2506.11175</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Hongyu Chen et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11175'&gt;http://arxiv.org/abs/2506.11175&lt;/a&gt;</description></item><item><title>Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages</title><link>http://arxiv.org/abs/2506.10292</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Ali Almutairi et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10292'&gt;http://arxiv.org/abs/2506.10292&lt;/a&gt;</description></item><item><title>RoCA: Robust Cross-Domain End-to-End Autonomous Driving</title><link>http://arxiv.org/abs/2506.10145</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Rajeev Yasarla et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10145'&gt;http://arxiv.org/abs/2506.10145&lt;/a&gt;</description></item><item><title>Description and Discussion on DCASE 2025 Challenge Task 2: First-shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring</title><link>http://arxiv.org/abs/2506.10097</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Tomoya Nishida et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10097'&gt;http://arxiv.org/abs/2506.10097&lt;/a&gt;</description></item><item><title>Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation</title><link>http://arxiv.org/abs/2506.09881</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Siyu Chen et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09881'&gt;http://arxiv.org/abs/2506.09881&lt;/a&gt; | Code: &lt;a href='https://github.com/anonymouse-9c53tp182bvz/vireo'&gt;https://github.com/anonymouse-9c53tp182bvz/vireo&lt;/a&gt;</description></item><item><title>Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments</title><link>http://arxiv.org/abs/2506.09552</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Fatemeh Mohammadi Amin et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09552'&gt;http://arxiv.org/abs/2506.09552&lt;/a&gt;</description></item><item><title>Evidential Deep Learning with Spectral-Spatial Uncertainty Disentanglement for Open-Set Hyperspectral Domain Generalization</title><link>http://arxiv.org/abs/2506.09460</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Amirreza Khoshbakht et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09460'&gt;http://arxiv.org/abs/2506.09460&lt;/a&gt;</description></item><item><title>Harmonizing and Merging Source Models for CLIP-based Domain Generalization</title><link>http://arxiv.org/abs/2506.09446</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Yuhe Ding et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09446'&gt;http://arxiv.org/abs/2506.09446&lt;/a&gt;</description></item><item><title>SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation</title><link>http://arxiv.org/abs/2506.09403</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Xinya Liu et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09403'&gt;http://arxiv.org/abs/2506.09403&lt;/a&gt; | Code: &lt;a href='https://github.com/hilab-git/srpl-sfda'&gt;https://github.com/hilab-git/srpl-sfda&lt;/a&gt;</description></item><item><title>ThinkQE: Query Expansion via an Evolving Thinking Process</title><link>http://arxiv.org/abs/2506.09260</link><pubDate>2025.06.10 00:00:00 GMT</pubDate><description>Yibin Lei et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09260'&gt;http://arxiv.org/abs/2506.09260&lt;/a&gt;</description></item><item><title>PlantBert: An Open Source Language Model for Plant Science</title><link>http://arxiv.org/abs/2506.08897</link><pubDate>2025.06.10 00:00:00 GMT</pubDate><description>Hiba Khey et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.08897'&gt;http://arxiv.org/abs/2506.08897&lt;/a&gt;</description></item><item><title>Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis</title><link>http://arxiv.org/abs/2506.08849</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Jingguo Qu et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.08849'&gt;http://arxiv.org/abs/2506.08849&lt;/a&gt; | Code: &lt;a href='https://github.com/jinggqu/nextgen-uia'&gt;https://github.com/jinggqu/nextgen-uia&lt;/a&gt;</description></item><item><title>FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching</title><link>http://arxiv.org/abs/2506.08518</link><pubDate>2025.06.10 00:00:00 GMT</pubDate><description>Sunny Gupta et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.08518'&gt;http://arxiv.org/abs/2506.08518&lt;/a&gt;</description></item><item><title>Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework</title><link>http://arxiv.org/abs/2506.08490</link><pubDate>2025.06.10 00:00:00 GMT</pubDate><description>Xiao Wei et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.08490'&gt;http://arxiv.org/abs/2506.08490&lt;/a&gt;</description></item><item><title>Low-resource domain adaptation while minimizing energy and hardware resource consumption</title><link>http://arxiv.org/abs/2506.08433</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Hernán Maina et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.08433'&gt;http://arxiv.org/abs/2506.08433&lt;/a&gt;</description></item><item><title>Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations</title><link>http://arxiv.org/abs/2506.08240</link><pubDate>2025.06.09 00:00:00 GMT</pubDate><description>Dongkyu Cho et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.08240'&gt;http://arxiv.org/abs/2506.08240&lt;/a&gt;</description></item><item><title>Play to Generalize: Learning to Reason Through Game Play</title><link>http://arxiv.org/abs/2506.08011</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Yunfei Xie et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.08011'&gt;http://arxiv.org/abs/2506.08011&lt;/a&gt; | Code: &lt;a href='https://github.com/yunfeixie233/vigal'&gt;https://github.com/yunfeixie233/vigal&lt;/a&gt;</description></item><item><title>Clustered Federated Learning via Embedding Distributions</title><link>http://arxiv.org/abs/2506.07769</link><pubDate>2025.06.09 00:00:00 GMT</pubDate><description>Dekai Zhang et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.07769'&gt;http://arxiv.org/abs/2506.07769&lt;/a&gt; | Code: &lt;a href='https://github.com/dkaizhang/emdcfl'&gt;https://github.com/dkaizhang/emdcfl&lt;/a&gt;</description></item><item><title>Beyond Benchmarks: A Novel Framework for Domain-Specific LLM Evaluation and Knowledge Mapping</title><link>http://arxiv.org/abs/2506.07658</link><pubDate>2025.06.09 00:00:00 GMT</pubDate><description>Nitin Sharma et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.07658'&gt;http://arxiv.org/abs/2506.07658&lt;/a&gt;</description></item><item><title>SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis</title><link>http://arxiv.org/abs/2506.07603</link><pubDate>2025.06.09 00:00:00 GMT</pubDate><description>Jianhui Wei et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.07603'&gt;http://arxiv.org/abs/2506.07603&lt;/a&gt;</description></item><item><title>Flowing Datasets with Wasserstein over Wasserstein Gradient Flows</title><link>http://arxiv.org/abs/2506.07534</link><pubDate>2025.06.09 00:00:00 GMT</pubDate><description>Clément Bonet et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.07534'&gt;http://arxiv.org/abs/2506.07534&lt;/a&gt; | Code: &lt;a href='https://github.com/clbonet/Flowing_Datasets_with_WoW_Gradient_Flows'&gt;https://github.com/clbonet/Flowing_Datasets_with_WoW_Gradient_Flows&lt;/a&gt;</description></item><item><title>Premise Selection for a Lean Hammer</title><link>http://arxiv.org/abs/2506.07477</link><pubDate>2025.06.09 00:00:00 GMT</pubDate><description>Thomas Zhu et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.07477'&gt;http://arxiv.org/abs/2506.07477&lt;/a&gt;</description></item><item><title>Affogato: Learning Open-Vocabulary Affordance Grounding with Automated Data Generation at Scale</title><link>http://arxiv.org/abs/2506.12009</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Junha Lee et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.12009'&gt;http://arxiv.org/abs/2506.12009&lt;/a&gt;</description></item><item><title>How Visual Representations Map to Language Feature Space in Multimodal LLMs</title><link>http://arxiv.org/abs/2506.11976</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Constantin Venhoff et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11976'&gt;http://arxiv.org/abs/2506.11976&lt;/a&gt;</description></item><item><title>Rethinking Multilingual Vision-Language Translation: Dataset, Evaluation, and Adaptation</title><link>http://arxiv.org/abs/2506.11820</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Xintong Wang et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11820'&gt;http://arxiv.org/abs/2506.11820&lt;/a&gt;</description></item><item><title>MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space</title><link>http://arxiv.org/abs/2506.11684</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Anshul Singh et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11684'&gt;http://arxiv.org/abs/2506.11684&lt;/a&gt;</description></item><item><title>VLM@school -- Evaluation of AI image understanding on German middle school knowledge</title><link>http://arxiv.org/abs/2506.11604</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>René Peinl et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11604'&gt;http://arxiv.org/abs/2506.11604&lt;/a&gt;</description></item><item><title>EasyARC: Evaluating Vision Language Models on True Visual Reasoning</title><link>http://arxiv.org/abs/2506.11595</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Mert Unsal et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11595'&gt;http://arxiv.org/abs/2506.11595&lt;/a&gt;</description></item><item><title>Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis</title><link>http://arxiv.org/abs/2506.11526</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Yuan Gao et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11526'&gt;http://arxiv.org/abs/2506.11526&lt;/a&gt;</description></item><item><title>Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs</title><link>http://arxiv.org/abs/2506.11515</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Xiao Xu et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11515'&gt;http://arxiv.org/abs/2506.11515&lt;/a&gt;</description></item><item><title>Taming Stable Diffusion for Computed Tomography Blind Super-Resolution</title><link>http://arxiv.org/abs/2506.11496</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Chunlei Li et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11496'&gt;http://arxiv.org/abs/2506.11496&lt;/a&gt;</description></item><item><title>On the Natural Robustness of Vision-Language Models Against Visual Perception Attacks in Autonomous Driving</title><link>http://arxiv.org/abs/2506.11472</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Pedram MohajerAnsari et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11472'&gt;http://arxiv.org/abs/2506.11472&lt;/a&gt;</description></item><item><title>Poutine: Vision-Language-Trajectory Pre-Training and Reinforcement Learning Post-Training Enable Robust End-to-End Autonomous Driving</title><link>http://arxiv.org/abs/2506.11234</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Luke Rowe et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11234'&gt;http://arxiv.org/abs/2506.11234&lt;/a&gt;</description></item><item><title>AIR: Zero-shot Generative Model Adaptation with Iterative Refinement</title><link>http://arxiv.org/abs/2506.10895</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Guimeng Liu et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10895'&gt;http://arxiv.org/abs/2506.10895&lt;/a&gt;</description></item><item><title>RationalVLA: A Rational Vision-Language-Action Model with Dual System</title><link>http://arxiv.org/abs/2506.10826</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Wenxuan Song et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10826'&gt;http://arxiv.org/abs/2506.10826&lt;/a&gt;</description></item><item><title>Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding</title><link>http://arxiv.org/abs/2506.10756</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Yuhang Zhang et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10756'&gt;http://arxiv.org/abs/2506.10756&lt;/a&gt;</description></item><item><title>IQE-CLIP: Instance-aware Query Embedding for Zero-/Few-shot Anomaly Detection in Medical Domain</title><link>http://arxiv.org/abs/2506.10730</link><pubDate>2025.06.13 00:00:00 GMT</pubDate><description>Hong Huang et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10730'&gt;http://arxiv.org/abs/2506.10730&lt;/a&gt;</description></item><item><title>GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning</title><link>http://arxiv.org/abs/2506.10639</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Xiaoyi Bao et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10639'&gt;http://arxiv.org/abs/2506.10639&lt;/a&gt;</description></item><item><title>Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning</title><link>http://arxiv.org/abs/2506.10575</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Chun-Mei Feng et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10575'&gt;http://arxiv.org/abs/2506.10575&lt;/a&gt;</description></item><item><title>LLMs Are Not Yet Ready for Deepfake Image Detection</title><link>http://arxiv.org/abs/2506.10474</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Shahroz Tariq et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10474'&gt;http://arxiv.org/abs/2506.10474&lt;/a&gt;</description></item><item><title>UrbanSense:AFramework for Quantitative Analysis of Urban Streetscapes leveraging Vision Large Language Models</title><link>http://arxiv.org/abs/2506.10342</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Jun Yin et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10342'&gt;http://arxiv.org/abs/2506.10342&lt;/a&gt;</description></item><item><title>Using Vision Language Models to Detect Students' Academic Emotion through Facial Expressions</title><link>http://arxiv.org/abs/2506.10334</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Deliang Wang et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10334'&gt;http://arxiv.org/abs/2506.10334&lt;/a&gt;</description></item><item><title>HalLoc: Token-level Localization of Hallucinations for Vision Language Models</title><link>http://arxiv.org/abs/2506.10286</link><pubDate>2025.06.12 00:00:00 GMT</pubDate><description>Eunkyu Park et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10286'&gt;http://arxiv.org/abs/2506.10286&lt;/a&gt;</description></item><item><title>Test-Time-Scaling for Zero-Shot Diagnosis with Visual-Language Reasoning</title><link>http://arxiv.org/abs/2506.11166</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Ji Young Byun et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11166'&gt;http://arxiv.org/abs/2506.11166&lt;/a&gt;</description></item><item><title>Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval</title><link>http://arxiv.org/abs/2506.10202</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Shubhashis Roy Dipta et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10202'&gt;http://arxiv.org/abs/2506.10202&lt;/a&gt;</description></item><item><title>Improving Personalized Search with Regularized Low-Rank Parameter Updates</title><link>http://arxiv.org/abs/2506.10182</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Fiona Ryan et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10182'&gt;http://arxiv.org/abs/2506.10182&lt;/a&gt;</description></item><item><title>A Navigation Framework Utilizing Vision-Language Models</title><link>http://arxiv.org/abs/2506.10172</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Yicheng Duan et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10172'&gt;http://arxiv.org/abs/2506.10172&lt;/a&gt;</description></item><item><title>One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence</title><link>http://arxiv.org/abs/2506.10157</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Michelle M. Li et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10157'&gt;http://arxiv.org/abs/2506.10157&lt;/a&gt;</description></item><item><title>ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs</title><link>http://arxiv.org/abs/2506.10128</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Xiyao Wang et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10128'&gt;http://arxiv.org/abs/2506.10128&lt;/a&gt;</description></item><item><title>VIBE: Can a VLM Read the Room?</title><link>http://arxiv.org/abs/2506.11162</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Tania Chakraborty et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.11162'&gt;http://arxiv.org/abs/2506.11162&lt;/a&gt;</description></item><item><title>Test-Time Adaptation for Generalizable Task Progress Estimation</title><link>http://arxiv.org/abs/2506.10085</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Christos Ziakas et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.10085'&gt;http://arxiv.org/abs/2506.10085&lt;/a&gt;</description></item><item><title>Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing</title><link>http://arxiv.org/abs/2506.09965</link><pubDate>2025.06.11 00:00:00 GMT</pubDate><description>Junfei Wu et al. | Paper: &lt;a href='http://arxiv.org/abs/2506.09965'&gt;http://arxiv.org/abs/2506.09965&lt;/a&gt; | Code: &lt;a href='https://github.com/antresearchnlp/vilasr'&gt;https://github.com/antresearchnlp/vilasr&lt;/a&gt;</description></item></channel></rss>